#!/usr/bin/env python
import sys
import os
import fnmatch
import sqlite3
import objgraph
import time

from clang.cindex import Index, TranslationUnit, CursorKind
from optparse import OptionParser, OptionGroup
from multiprocessing import Pool, Lock, Manager, Queue

class SourceFile:
    def __init__(self):
        self.extensions = ['*.c', '*.h', '*.C', '*.H',
                           '*.c++', '*.cc', '*.cp', '*.cpp', '*.cxx',
                           '*.h++', '*.hh', '*.hp', '*.hpp', '*.hxx']

    def locate(self, root):
        '''Locate all C/C++ files matching extensions attribute.
        '''
        for path, dirs, files in os.walk(os.path.relpath(root)):
            for extension in self.extensions:
                for filename in fnmatch.filter(files, extension):
                    yield os.path.join(os.path.relpath(path), filename)


class TagDB:
    def __init__(self, outputFile='tags.db'):
        self.db = sqlite3.connect(outputFile)
        cursor = self.db.cursor()
        cursor.execute("PRAGMA synchronous = OFF;")
        cursor.execute("PRAGMA journal_mode = MEMORY;")
        cursor.execute("""CREATE TABLE IF NOT EXISTS tags (
        location   TEXT,
        line       INT,
        column     INT,
        offset     INT,
        start_line INT,
        start_col  INT,
        end_line   INT,
        end_col    INT,
        kind_name  TEXT,
        type_name  TEXT,
        spelling   TEXT,
        display    TEXT,
        is_def     INT,
        def        TEXT,
        is_static  INT,
        is_ref     INT,
        ref        TEXT,
        usr        TEXT);""")
        self.db.commit()

    def __del__(self):
        self.db.close()

    def create_index(self):
        cursor = self.db.cursor()
        cursor.execute("CREATE INDEX IF NOT EXISTS location_index ON tags(location);")
        cursor.execute("CREATE INDEX IF NOT EXISTS location_line_index ON tags(location,line);")
        cursor.execute("CREATE INDEX IF NOT EXISTS kind_index ON tags(kind_name);")
        cursor.execute("CREATE INDEX IF NOT EXISTS type_index ON tags(type_name);")
        cursor.execute("CREATE INDEX IF NOT EXISTS spelling_index ON tags(spelling);")
        cursor.execute("CREATE INDEX IF NOT EXISTS def_index ON tags(def);")
        cursor.execute("CREATE INDEX IF NOT EXISTS ref_index ON tags(ref);")
        cursor.execute("CREATE INDEX IF NOT EXISTS usr_index ON tags(usr);")            
        self.db.commit()
    
    def persist(self, ast):
        cursor = self.db.cursor()
        cursor.executemany("INSERT INTO tags VALUES(?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?)", ast)
        self.db.commit()

class Options:
    def __init__(self):
        self.autoInclude = True
        self.showIDs = False
        self.verbose = False
        self.maxDepth = 0
        self.jobs    = 1
        self.outputFile = 'tags.db'


class Crange:
    "Class Crange"

    def __init__(self):
        self.opts  = Options()
        self.args  = list()  # Arguments passed from the command line.
        self.ast   = dict()  # List containing nodes present in a source file.

    def __del__(self):
        self.ast.clear()
        
    def debug(self, message):
        if self.opts.verbose:
            print message

    def get_diag_info(self, diag):
        return { 'severity' : diag.severity,
                 'location' : diag.location,
                 'spelling' : diag.spelling,
                 'ranges' : diag.ranges,
                 'fixits' : diag.fixits }

    def get_cursor_id(self, cursor, cursor_list = []):
        if not self.opts.showIDs:
            return None
                
        if cursor is None:
            return None

        # FIXME: This is really slow. It would be nice if the index API exposed
        # something that let us hash cursors.
        for i,c in enumerate(cursor_list):
            if cursor == c:
                return i
        cursor_list.append(cursor)
        return len(cursor_list) - 1

    ###### Node's key reference:
    # 1.  location   : node's filename    
    # 2.  line       : location line number
    # 3.  column     : location column number
    # 4.  offset     : location offset
    # 5.  start_line : extent start line
    # 6.  start_col  : extent start column
    # 7.  end_line   : extent end line
    # 8.  end_col    : extent end column
    # 9.  kind_name  : node kind name
    # 10. type_name  : node type name
    # 11. spelling   : spelling
    # 12. display    : display
    # 13. is_def     : node is a definition?
    # 14. def        : USR of node's definition.
    # 15. is_static  : node is a static method?
    # 16. is_ref     : node is a reference?
    # 17. ref        : USR of node's that is referenced.
    # 18. usr        : USR of this node.
    ######
    def node_to_tuple(self, node):
        definition = node.get_definition().get_usr() if node.get_definition() else None
        referenced = node.referenced.get_usr() if node.referenced else None
        return (str(node.location.file),
                node.location.line,
                node.location.column,
                node.location.offset,
                node.extent.start.line,
                node.extent.start.column,
                node.extent.end.line,
                node.extent.end.column,
                node.kind.name,
                node.type.kind.name,
                node.spelling,
                node.displayname,
                node.is_definition(),
                definition,
                node.is_static_method(),
                node.kind.is_reference(),
                referenced,
                node.get_usr())

    def get_info(self, node, depth=0):
        if self.opts.maxDepth is not None and depth >= self.opts.maxDepth:
            children = None
        else:
            children = [self.get_info(c, depth+1)
                        for c in node.get_children()]
            
        loc = str(node.location.file) if node.location.file else None
        if loc is not None:
            if loc not in self.ast:
                self.ast[loc] = list()
            self.ast[loc].append(self.node_to_tuple(node))


tagdb = TagDB('tags.db')
indexed_files = set()

def dbkeeper(queue, opts):
    while True:
        ast = queue.get()
        if ast == 0:
            break
        else:
            for loc, nodes in ast.iteritems():
                if loc not in indexed_files:
                    print "Indexing %s nodes for %s (qsize: %s)" % (len(nodes), loc, queue.qsize())
                    indexed_files.add(loc)
                    tagdb.persist(nodes)

def worker(worker_params):
    source, count, queue, opts, args = worker_params
    index  = Index.create()
    c      = Crange()
    c.opts = opts
    c.args = args
    clang_line = [source, '-Iinclude']
    try:
        tu = index.parse(None, clang_line)
        c.get_info(tu.cursor)
        if len(c.ast) > 0:
            queue.put(c.ast)
    except Exception as e:
        print "Error parsing %s: %s" % (source, e.message)
    c.debug("%s (%s)" % (source, count+1))

def spawn_workers(opts, args):
    root = args[0]
    sfo  = SourceFile()
    pool = Pool(opts.jobs)
    manager = Manager()
    queue   = manager.Queue()

    # Fire dbkeeper process.
    pool.apply_async(dbkeeper, (queue, opts))

    # Then fire AST worker processes.
    worker_params = ((s, count, queue, opts, args) for count,s in enumerate(sfo.locate(root)))
    pool.map(worker, worker_params)

    # Kill the dbkeeper loop.
    queue.put(0)

    # Close and reap the worker processes.
    pool.close()
    pool.join()

if __name__ == '__main__':
    parser = OptionParser("usage: %prog [options] {directory} [clang-args*]")
    parser.add_option("-a", "--auto-include", dest="autoInclude" ,default=False, action="store_true",
                      help="Automatically detect and add include paths (-Isrc/include)")
    parser.add_option("-i", "--show-ids", dest="showIDs", default=False, action="store_true",
                      help="Don't compute cursor IDs (very slow)")
    parser.add_option("-d", "--max-depth", dest="maxDepth", metavar="N", type="int", default=None,
                      help="Limit cursor expansion to depth N")
    parser.add_option("-o", "--output", dest="outputFile", metavar="FILE", type="string", default="tags.db",
                      help="Create tags database in FILE")
    parser.add_option("-j", "--jobs", dest="jobs", metavar="JOBS", type="int", default=1,
                      help="Specifies the number of jobs to run simultaneously")
    parser.add_option("-v", "--verbose", dest="verbose", default=False, action="store_true",
                      help="Enable verbose mode")
    parser.disable_interspersed_args()
    opts, args = parser.parse_args()
    root = args[0]        

    if len(args) == 0:
        parser.error('Invalid number of arguments')

    if not os.path.isdir(root):
        parser.error("%s is not a directory" % root)
    else:
        spawn_workers(opts, args)
        print "Generating indexes"
        tagdb = TagDB(opts.outputFile)
        tagdb.create_index()
